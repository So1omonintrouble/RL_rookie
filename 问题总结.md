## 230301

>  先计划，再动手

### 整体目标

1. 找回昨天还能快速训练的版本，搞清楚是什么让模型训练速度**变慢**
   具体操作：尝试把状态空间改回来，用DDPG的算法训练PPO的模型，需要修改的地方有：
   1. 环境名称
   2. 导入状态
      结果：状态空间不是影响训练速度的原因
      交叉验证，使用PPO算法执行DDPG模型，速度很快，证明可能是算法的问题，或者是神经网络的问题
      再设一个m程序m230301_test_PPO，用PPO的算法，DDPG的神经网络结构训练，对比神经网络结构后发现DDPG神经网络还不如PPO的复杂，因此可以确定，是算法本身的问题，
      问题：如何让DDPG算法训练变快
      1. 在仿真模型中尝试加入**delay**
      2. 可能遇到了与DQN相同的问题，试着从DQN模型里面找找答案
2. 一步一步来，，搞清楚为什么昨天训练会在**稳定时候中断**
3. **variance**这个量到底是怎么大幅度影响到训练的（没记录数据，可能与第2条相关
4. 神经网络输出**动作值超出上下限**到底如何解决，有什么解决方法，怎样就表示解决了
5. 训练加入动作惩罚时，每个episode奖励为-1k显著增加，会是什么原因（怎么改动作惩罚的权重都会出现-1k）
6. **神经网络层数**到底怎么设计才合适，越深越容易收敛吗
7. 神经网络输入层需不需要做**归一化处理**，需要的话，mean，variance，min，max如何设置
8. 现阶段由于状态的**不完全可观性**，是否需要使用具有记忆单元的神经网络与DRL相结合
9. 亦或是换个方法？直接用Agent替代PSS**输出直流电压参考值的附加值而不是替代k值**
10. 或者再改进下方法，在0.01s时发生振荡，经过0.5s，检测是否存在振荡（与目标值只差超出100就存在），有的话就停止，没有的话经过一小段时间也停止，**降低仿真时间**

