## 0306可干

1. 找回昨天还能快速训练的版本，搞清楚是什么让模型训练速度**变慢**
   具体操作：尝试把状态空间改回来，用DDPG的算法训练PPO的模型，需要修改的地方有：
   1. 环境名称
   2. 导入状态
      结果：状态空间不是影响训练速度的原因
      交叉验证，使用PPO算法执行DDPG模型，速度很快，证明可能是算法的问题，或者是神经网络的问题
      再设一个m程序m230301_test_PPO，用PPO的算法，DDPG的神经网络结构训练，对比神经网络结构后发现DDPG神经网络还不如PPO的复杂，因此可以确定，是算法本身的问题，
      问题：如何让DDPG算法训练变快
      1. 在仿真模型中尝试加入**delay**
      2. 可能遇到了与DQN相同的问题，试着从DQN模型里面找找答案