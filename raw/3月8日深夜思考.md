## 3月8日深夜思考

### 新方法

用TD3强化学习算法调出一个比较好的静态k之后，再设计一个强化学习agent，让这个agent对k值进行微调。

**微调**

微调的方式一开始可以设为离散动作，使用DQN或PPO算法就可以实现，刚开始动作值设定为+0.001，0，-0.001比较简单，如果有效果，则可以把离散动作值设置更多一点，如果没效果，就缩小这个范围直到这个范围学出更好的奖励

### TD3算法不收敛的原因

不收敛的学习曲线如下

![image-20230309140803403](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230309140803403.png)

TD3的actor网络只有一个可学习参数，也就是k值，初始设定比较小，随着训练会一直提高

当其达到效果最好的地方，但此时q值和实际的奖励的偏差让actor网络继续更新，所以k值继续增加，最终导致失稳

actor网络更新时损失函数的梯度计算
$$
\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\theta log\pi_\theta(a_{i,t}|s_{i,t})(r(s_t,a_t)+\hat{V}(s_{t+1})-\hat{V}(s_t))
$$
策略更新
$$
\theta=\theta+\alpha \nabla J(\theta)
$$
其中$\pi_\theta$为actor输出值

对$\nabla_\theta log \pi_\theta$求梯度：
$$
\nabla_\theta log \pi_\theta=\frac{1}{\pi_\theta}\nabla\pi_\theta
$$
前面一定是正的，对actor神经网络反向传播，由于actor的预测predict函数为参数的绝对值乘上输入，**~~因此~~**$\nabla\pi_\theta$与$\pi_\theta$同号，w为负所以，因此前面这一整体恒为负数

而$r(s_t,a_t)$是从环境中得到的，根据TD3算法对应的奖励环境，得到的奖励都是负的，所以$r(s_t,a_t)$一定为负，至于说$\hat{V}(s_{t+1})-\hat{V}(s_t)$，根据学习曲线可知，Q0一直都是0附近，所以可以直接忽略，因此后面这一整体为负数

综上所述，$\nabla J(\theta)$一定是正的，而当w为正时，$\nabla J(\theta)$又恒为负的，$\alpha$是神经网络学习率，恒正且不变，所以说$\theta$在一直变小，再看看训练中间过程的实际设定的神经网络可学习参数，所以他应该在w=0出收敛

通过代码：

```matlab
actor = getActor(saved_agent);
parameters = getLearnableParameters(actor);
K = abs(parameters{1}(1))
w = parameters{1}(1)
```

<img src="https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230309140424608.png" alt="image-20230309140424608" style="zoom:50%;" />

这就说明，这整个学习过程，其实就是一个w参数无脑降低的过程。

**结论**

critic网络没训练好，导致Q0跟不上实际奖励，进一步导致学习曲线跑飞