

# 230213-230302

## 学习

### 新版matlab水库水位控制的例子
![image-20230216140300154](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216140300154-16777432913434.png)

```matlab
   blk = 'rlWatertankStepInput/WaterLevelStepResponse';
   generateRewardFunction(blk)
```
1. 使用这样的代码就可以生成奖励函数，WaterLevelStepResponse是Simulink的一个模块，它参数可以自由设置
      ![image-20230216141107654](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216141107654.png)
   规定了在指定时间内，所期望的目标控制范围![image-20230216140830977](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216140830977-167774356272210.png)生成的代码较为复杂，其中比较核心的代码应该是

```matlab
Penalty = sum(exteriorPenalty(x,Block1_xmin,Block1_xmax,'quadratic'));
```

直接计算是否在目标范围内，并随着与目标距离的提高而提高惩罚

其他的主要就是用来计算对应时间对应最大最小值了

2. 状态空间选取

![image-20230216144024423](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216144024423-16777466148894.png)

同样是面对一段时间序列的水位变化波形，刚开始学强化学习时候也有个水库水位控制，跟新版的相比主要差别在于状态空间的选取，旧版主要选用误差积分，误差，水位高度三个量，新版采用6个量，水位高度本身，对该信号进行4次延迟得到的5个信号以及误差，感觉这样就可能观测到更多的信息来进行学习

### 从稳态开始训练

怀疑是0-1s本应该无效的数据被强化学习Agent记录的缘故，状态值对应的是一段暂态波形，而没有设置奖励，故奖励为0，而且奖励最大也就是0，因此**0-1s数据本身就会误导Agent进行学习**，所以怎么将0-1s数据除去目前来看较为重要。

因此如果直接从稳态开始应该就可以避免这个情况

> 230215：可以通过记录稳态所有状态，下次仿真直接从稳态开始的方法来解决
>
> 调整求解器，目前仿真不规则波形已除去
>
> 加入强化学习agent，并添加变量，可以简单实现强化学习Agent的输出
>
> ![image-20230221131201105](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230221131201105-16777471432267.png)
>
> 以上实际时间为4-7s有功功率波形
>
> ![image-20230221131321656](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230221131321656-16777471432269.png)

### 明确了PSS的投入

PSS的投入应该是当系统处于稳态时，而不是出现振荡之后才投入。且其本身的投入也会引发系统振荡，因此在稳态阶段要进行预先调整的量有2个，闭环控制器带宽和PSS系统的投入，两个分别在1s投入

## 训练结果

### 230226

重新搭建了模型，但仍然无法收敛

采取了一些解决方法

- [x] 把动作输出的限幅器去掉，同时加入中断函数，判断何时中断
  ![image-20230302180848086](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302180848086.png)
- [x] 修改神经网络参数
- [x] 对神经网络输出的动作值，其平均值和方差进行裁剪，以保持在某个范围内
  ![image-20230302181049365](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302181049365.png)
- [ ] 设计惩罚函数，因振荡没有完全抑制而惩罚
- [x] 修改agentOpt，仿真错误而导致训练中断

| 学习曲线                                                     | 波形                                                         | 奖励                                                         | 动作值                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20230302182108596](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302182108596-167775267455925.png) | ![image-20230302182440920](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302182440920.png) | ![image-20230302182445512](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302182445512.png) | ![image-20230302182552095](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302182552095.png) |

### 230227

将原来PPO模型另存为一个新的Simulink模型，名为DDPG230227

由于DDPG的优势可能不在于对高维度的状态空间的处理，所以如果效果不理想的情况下，可能会修改状态量。

在此之前xFinal，也就是系统稳态时候的状态量是可以使用PPO的xFinal的，

不能用PPO的xFinal，因为模型名称已经改变了，新生成一个xFinal，取名为xFinal230227.mat

![image-20230302183701996](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302183701996.png)

### 230228

修改神经网络，继续训练

| ![image-20230302183747927](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302183747927.png) | ![image-20230302184408619](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302184408619.png) | ![image-20230302184519236](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302184519236.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |

- [ ] 打算加入一个振荡没有得到抑制的额外惩罚

- [ ] 由于仅使用有功功率P势必会导致系统的部分可观性，使用有记忆的神经网络

- [x] **或者是根据系统所有状态变量，重新设计强化学习结构**
  根据之前搭建的完全平均化模型中设计的状态空间，选取其状态量当作强化学习状态空间

- [ ] 给神经网络输入层加入标准化nomal一下（强化学习不需要归一化文章https://zhuanlan.zhihu.com/p/210761985）

### 230301

**主要是对问题3的探索**

具体见onenote 230301



## 问题

### 问题1：仿真时出现不规则波形

把上述方法用到有Agent的Simulink模型中，仍存在问题，应该是Simulink强化学习模块的问题

| 没有强化学习模块<img src="https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216160653592.png" alt="image-20230216160653592" style="zoom: 33%;" /> | ![image-20230216160304520](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216160304520.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 有强化学习模块（包括奖励函数，状态空间生成）<img src="https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216160610557.png" alt="image-20230216160610557" style="zoom: 25%;" /> | ![image-20230216160129680](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216160129680.png) |
| 甚至只加一个RL Agent模块都有问题<img src="https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216160544245.png" alt="image-20230216160544245" style="zoom: 50%;" /> | ![image-20230216160421158](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230216160421158.png) |

同时经过对比发现加入unit delay也会产生类似这种情况

> 230220：可以通过修改求解器为23tb来解决😂

### 问题2 随机策略PPO的问题

![image-20230302182949840](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302182949840.png)

PPO算法处理连续的动作空间时候，它总是输出一个平均值和一个方差，只要这个方差存在且不等于0，就即使训练收敛了，它在应用的时候，稳定性感觉还是不高吧

## 230228训练时的问题

由于是确定性策略，感觉模型随时可能通过调节一个或其他的细节就可以收敛，同时，28日的自己有些浮躁，没有对过程进行记录。当天对模型进行了多次修改和训练，每次都存在相应的问题，0301对上一天的训练做出了总结。

### 问题3 230228的训练明显变慢

由于设置了中断函数，导致很多次的训练刚开始就已经结束，误以为速度还与之前一样，再次观察时发现训练速度明显变慢，因此进行相关探索。

具体操作：尝试把状态空间改回来，用DDPG的算法训练PPO的模型，从而可以交叉验证，需要修改的地方有：

1. 环境名称

2. 导入状态

   结果：状态空间不是影响训练速度的原因
   交叉验证，使用PPO算法执行DDPG模型，速度很快，证明可能是算法的问题，或者是神经网络的问题
   再设一个m程序m230301_test_PPO，用PPO的算法，DDPG的神经网络结构训练，对比神经网络结构后发现DDPG神经网络还不如PPO的复杂，因此可以排除神经网络结构的问题，是算法本身的问题，

   **问题：如何让DDPG算法训练变快**

   1. 在仿真模型中尝试加入**delay**
   2. 与DQN的速度慢有些相似，但DQN的训练速度慢的问题当时也是没有解决

### 问题4 训练在稳定时中断，对应奖励接近0

情况：提高DDPG的噪声的**variance**参数，从0.4调高，调高到4，就会出现这个问题

而且模型中断应该也是当目标值超出期望范围内才中断，这个直接在稳态就断了（会不会是模型出错了😂直接断了）

**variance**这个量是怎么大幅度影响到训练的

![image-20230302191515119](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302191515119.png)

### 问题5 神经网络输出**动作值超出上下限**

![image-20230302192101200](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302192101200.png)可能就是动作值超出上限导致的

### 问题6 （奇怪的情况）加入动作惩罚时，训练时频繁出现奖励为-1k的情况

而且当对这个动作惩罚的权重改变时，-1k奖励的情况，基本没变

### 问题7 **神经网络层数**到底怎么设计才合适

越深越容易收敛吗

### 问题8 **神经网络输入层**需不需要做**归一化处理**

需要的话，mean，variance，min，max如何设置

（强化学习不需要归一化文章https://zhuanlan.zhihu.com/p/210761985）

### 问题9 状态的**不完全可观性**对收敛的影响

是否需要使用具有记忆单元的神经网络与DRL相结合

![image-20230302193700356](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230302193700356.png)

> 《电力系统优化控制中强化学习方法应用及挑战》论文

## 目前想到的一些还没做的方法

1. 直接用Agent替代PSS**输出直流电压参考值的附加值而不是替代k值**

2. 改进下方法，在0.01s时发生振荡，经过0.5s，检测是否存在振荡（与目标值只差超出100就存在），有的话就停止，没有的话经过一小段时间也停止，**降低仿真时间**
3. 先将简单系统建模，在已知系统模型的情况下，也就是通过有模型强化学习方法进行学习，学到的Agent如果有效，简单系统发生工况变化，此时在学好的Agent基础上继续学习，中间可以有个过渡退火的过程，即将工况进行一定速度的迁移，如果学习曲线下降，则降低速度，直到可以稳步提高学习曲线

## 230303读论文

**《电力系统优化控制中强化学习方法应用及挑战》**

主要统计了一下相关论文应用强化学习模型的马尔可夫属性，系统状态的可观测性，最优性，次优性保证

**《Fusion of Microgrid Control with Model-free Reinforcement Learning: Review and Vision》**

可能更偏重于强化学习用于微电网的什么地方，把强化学习放到微电网哪个环节上去了

然后是强化学习怎么放上去的，有参数调节，模型辨识，附加生成信号，替代控制器，
