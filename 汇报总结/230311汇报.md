# 230311 汇报

## 总结

**问题**：训练很慢的问题仍然没有解决

**新方法**：先找一个静态最优的动作值，然后引入强化学习Agent在此动作值基础上进行附加控制

---

**新发现**：采样时间可能对强化学习训练有较大的影响

## 3.9之前

### 问题

经过尝试，TD3，DDPG，DQN算法这种确定性策略强化学习算法均会出现训练很慢的情况，且添加延迟信号没有作用

### 新方法

**想法：** 由于非线性控制器之于线性控制器的优势，恒定的控制参数的性能上限比不上变化的控制参数

**操作：** **调出一个比较好的静态k**之后，再设计一个强化学习Agent，**这个Agent是k值的附加信号**。

> 微调的方式一开始可以设为离散动作，使用DQN或PPO算法就可以实现，刚开始动作值设定为+0.001，0，-0.001比较简单，如果有效果，则可以把离散动作值设置更多一点，如果没效果，就缩小这个范围直到这个范围学出更好的奖励

**具体：**

针对调出较好的**静态k值**，除了人工手调，还可以借助强化学习算法进行，从matlab的强化学习调PI参数的示例中学的，具体方法如下：

> 从整体来看，是用强化学习Agent替代了PI控制器，但训练的结果又能根据神经网络结构得到$k_P$,$k_I$的具体参数，
>
> 细节在于，另外设计了一个PI神经网络层，本质上是一个全连接层，predict预测时所有权重都取绝对值
>
> ![image-20230310214228927](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230310214228927.png)
>
> ![image-20230310214255932](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230310214255932.png)

> 如果放到目前正在做的振荡抑制中，由于目前PSS只考虑k值，因此设计一个PSS神经网络层，其输入数量为1，输出数量为1，可学参数数量为1，即k值。
>
> ![image-20230310214320669](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230310214320669.png)
>
> ![image-20230310214332479](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230310214332479.png)



### 新方法结果

可以达到手调的最好的k值附近，但不稳定

![image-20230308185019395](https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230308185019395-16784563121141.png)

初步判断是因为critic神经网络的问题，后面会尝试以此为目标来调试

#### 尝试对不收敛进行推导（未成功）

TD3的actor网络只有一个可学习参数，也就是k值，初始设定比较小，随着训练会一直提高

当其达到效果最好的地方，但此时Q0值和实际的奖励偏差让actor网络继续更新，所以k值继续增加，最终导致失稳

actor网络更新时损失函数的梯度计算
$$
\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\theta log\pi_\theta(a_{i,t}|s_{i,t})(r(s_t,a_t)+\hat{V}(s_{t+1})-\hat{V}(s_t))
$$
策略更新
$$
\theta=\theta+\alpha \nabla J(\theta)
$$
其中$\pi_\theta$为actor输出值

对$\nabla_\theta log \pi_\theta$求梯度：
$$
\nabla_\theta log \pi_\theta=\frac{1}{\pi_\theta}\nabla\pi_\theta
$$
对actor神经网络反向传播，由于actor的预测predict函数为参数的绝对值乘上输入，$\nabla\pi_\theta$与$\pi_\theta$同号，w为负，所以前面这一整体恒为负数

而$r(s_t,a_t)$是从环境中得到的奖励，根据TD3算法对应的奖励环境，得到的奖励都是负的，所以$r(s_t,a_t)$一定为负，至于说$\hat{V}(s_{t+1})-\hat{V}(s_t)$，根据学习曲线可知，Q0一直都是0附近，所以可以直接忽略，因此后面这一整体为负数

综上所述，w为负时，$\nabla J(\theta)$一定是正的，而当w为正时，$\nabla J(\theta)$又恒为负的，$\alpha$是神经网络学习率，恒正且不变，所以说$\theta$在一直变小，再看看训练中间过程的实际设定的神经网络可学习参数，所以他应该在w=0出稳定。（推导失败😂）

> 通过代码判断w值正负：
>
> <img src="https://cdn.jsdelivr.net/gh/So1omonintrouble/gitpush/image/image-20230309140424608-16784565146954.png" alt="image-20230309140424608" style="zoom:50%;" />

## 3.10看文档

### 发现

翻了翻matlab强化学习文档，看到个控制永磁同步电机的matlab范例，电机分外环转速pi控制器，内环电流pi控制器，他是用强化学习替代了内环控制，采样时间是2e-4，我用的才1e-2，感觉学不会很大程度上跟这个有关。

然后就是这个范例的奖励函数设计的也挺好的感觉


$$
r_{t}=-\left(Q_{1} * \mathrm{id}_{\text {error }}^{2}+Q_{2} * \mathrm{iq}_{\text {error }}^{2}+R * \sum_{j} u_{t-1}^{j}{ }^{2}\right)-100 d
$$


其中$Q_1 = Q_2 = 5 ， R =0.1$

